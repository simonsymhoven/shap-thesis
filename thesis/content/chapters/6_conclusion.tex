\chapter{Fazit}

Das Fazit der vorliegenden Arbeit reflektiert die erzielten Erkenntnisse 
aus der Anwendung des linearen Regressionsmodells zur Vorhersage der Laufzeiten von GPU-Kernen. 
Durch die Integration von SHAP-Werten konnte eine tiefergehende Interpretation der Modellvorhersagen 
erreicht werden, die über die traditionelle statistische Analyse hinausgeht. Es wurde demonstriert,
dass die Koeffizienten des linearen Modells einen direkten Einblick in die Beziehung zwischen den 
unabhängigen Variablen und der Zielvariable bieten, während die SHAP-Werte es ermöglichen, 
diese Beziehungen auf einer granularen Ebene zu interpretieren.

Die Visualisierungen, insbesondere der Waterfall- und Beeswarm-Plots,
haben eine klare und intuitive Darstellung der Merkmals-Beiträge ermöglicht, die für die 
Verständlichkeit des Modellverhaltens von entscheidender Bedeutung ist. Die Arbeit hat auch aufgezeigt, 
dass die lokale Interpretation, obwohl sie präzise und aufschlussreich für einzelne Vorhersagen ist, 
durch die globale Perspektive ergänzt werden muss, um allgemeingültige Schlussfolgerungen über das Modellverhalten zu ziehen.

Insgesamt hat die Untersuchung gezeigt, dass die Kombination aus linearen Regressionsmodellen und SHAP-basierter 
Interpretation ein leistungsstarkes Werkzeug darstellt, um sowohl die Vorhersagegenauigkeit als auch die Transparenz 
und Nachvollziehbarkeit von maschinellen Lernmodellen zu verbessern. Die gewonnenen Einsichten können genutzt werden, 
um das Modell weiter zu verfeinern, und bieten eine Grundlage für fundierte Entscheidungen bei der Optimierung von GPU-Kerneln. 

Gleichzeitig werden jedoch auch die Grenzen und Herausforderungen dieser Methode deutlich.
Kritisch betrachtet, können Shapley-Werte in bestimmten Kontexten zu einer Überinterpretation führen, 
insbesondere wenn die Beziehungen zwischen den Merkmalen komplex und nicht-linear sind. 
Die Berechnung von Shapley-Werten kann zudem rechnerisch intensiv sein, vor allem bei Modellen mit einer großen Anzahl von Merkmalen. 
Die Entwicklung effizienterer Algorithmen und die Nutzung fortschrittlicher Rechenressourcen, wie GPU-basiertes Computing, 
könnten die Anwendbarkeit von SHAP-Werten in der Praxis erheblich erweitern. Dies würde es ermöglichen, auch bei umfangreicheren und 
komplexeren Modellen eine detaillierte Erklärbarkeit zu gewährleisten.

Die Anwendung von Shapley-Werten im maschinellen Lernen bietet vielfältige Möglichkeiten, die weit über die in dieser Arbeit behandelten Aspekte hinausgehen. 
Zukünftige Forschungen und Entwicklungen könnten sich in mehrere Richtungen erstrecken, um die Grenzen und Potenziale von SHAP-Werten weiter auszuloten und zu erweitern.

Eine wichtige Erweiterung der SHAP-Wert-Analyse ist die Einbeziehung von Interaktionen zwischen Merkmalen. Viele reale Datensätze weisen 
komplexe Beziehungen und Abhängigkeiten zwischen ihren Merkmalen auf. Diese Interaktionen explizit zu modellieren und in die Berechnung der SHAP-Werte zu integrieren, 
würde zu einer noch genaueren und realistischeren Interpretation der Modellvorhersagen führen.

