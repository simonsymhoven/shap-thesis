\chapter{Einleitung}

In einer Ära, in der datengetriebene Entscheidungsfindung und automatisierte Modelle zunehmend an Bedeutung gewinnen,
wird die Fähigkeit der Erklärung und Interpretation dieser Modelle immer wichtiger. 
Obwohl maschinelles Lernen und künstliche Intelligenz beachtliche Fortschritte erzielt haben, ist es nicht allein die Komplexität 
der Modelle, die Verständnisschwierigkeiten bereitet, sondern vor allem die Transparenz ihrer Entscheidungsprozesse. 
Während der grundlegende Aufbau eines Modells, wie etwa eines neuronalen Netzes mit einer überschaubaren Anzahl an Neuronen, 
durchaus verständlich sein kann, bleibt die spezifische Art und Weise, wie das trainierte Modell Daten verarbeitet und zu Vorhersagen gelangt,
oft undurchsichtig und schwer nachvollziehbar \cite[S. 4f]{Molnar_2023}. Selbst einfache Modelle, wie eine lineare Regression,
können durch Transformationen der Eingabemerkmale oder der Zielvariable schwer zu interpretieren sein \cite[S. 6]{Molnar_2023}. 
Warum ein bestimmtes Modell eine entsprechende Entscheidung getroffen hat oder wie sich die einzelnen
Merkmale auf die Vorhersagen auswirken, wird zu einer zentralen Frage für das Verständnis der zugrundeliegenden Mechanismen. 
Shapley-Werte bieten eine Möglichkeit, die \glqq{}Black Box\grqq{} der Modelle zu öffnen und Einblicke 
in die Modellentscheidung zu gewinnen \cite[S. 215f]{Molnar_2022}. 

Diese Masterarbeit widmet sich einer tiefgehenden Untersuchung der Shapley-Werte und erforscht ihr Potenzial für die Interpretation
von Machine Learning-Modellen, insbesondere linearer Modelle, sowie ihre Anwendbarkeit auf reale Datensätze.

Die Arbeit beginnt mit einer Einführung in die Shapley-Werte, indem ihre historischen Ursprünge 
und ihre Verbindung zur kooperativen Spieltheorie beleuchtet werden. Die theoretischen Grundlagen dieser Werte werden 
ergründet und ihre Bedeutung für die Interpretation von Modellen untersucht.

Anschließend wird der Fokus auf die Anpassung und Erweiterung der Shapley-Werte für maschinelles Lernen gelegt. 
Hierbei wird betrachtet, wie Shapley-Werte modifiziert werden können, um tiefere Einsichten in die Gewichtung 
und Bedeutung einzelner Merkmale innerhalb komplexer Machine Learning-Modelle zu ermöglichen. Dabei wird ein Vergleich 
mit bestehenden Methoden, wie der Permutation der Merkmalrelevanz, gezogen, wodurch die Einzigartigkeit und der Mehrwert der 
Shapley-Werte hervorgehoben wird.

Durch die Analyse eines realen Datensatzes wird die Methodik in der Praxis demonstriert. Die Fallstudie zur Vorhersage der 
Druckfestigkeit von Beton bietet dabei ein konkretes Beispiel, anhand dessen die Nuancen und die Tiefe der durch SHAP 
ermöglichten Analysen veranschaulicht werden.

Abschließend werden die gewonnenen Erkenntnisse zusammengefasst und reflektiert. Es wird speziell die Rolle und das 
Potenzial der Shapley-Werte in diesem Kontext beleuchtet.