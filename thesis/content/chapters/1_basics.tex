\section{Der Ursprung der Shapley-Werte in der kooperativen Spieltheorie}

Der Ursprung der Shapley-Werte liegt in der kooperativen Spieltheorie, einem fundamentalen Zweig der Spieltheorie. 
Dieser Bereich beschäftigt sich mit der Analyse von Situationen, in denen Akteure zusammenarbeiten, um gemeinsame Ziele zu erreichen. 
Zentrales Anliegen ist dabei die gerechte Verteilung der resultierenden Gewinne unter den Akteuren. Ein Schlüsselkonzept dieser Theorie 
ist die sogenannte charakteristische Funktion, welche die Bewertung der Gewinnverteilung einer Koalition von Akteuren ermöglicht \cite[S. 12]{Molnar_2023}.

Die Shapley-Werte, entwickelt von Lloyd Shapley in den 1950er Jahren, bieten einen methodischen Ansatz, um den individuellen Beitrag 
eines jeden Akteurs zur kooperativen Zusammenarbeit gerecht zu bewerten\footnote{Erstmalige Veröffentlichung: \glqq{}A Value for n-Person Games\grqq{}, \cite[S. 307-318]{Shapley+1953+307+318}}. 
Dies geschieht durch die Durchschnittsbewertung der Beiträge 
über sämtliche mögliche Koalitionen hinweg. Diese Methode erweist sich als äußerst nützlich, um eine gerechte und rationale Verteilung 
von Gewinnen in vielfältigen Szenarien zu ermöglichen, sei es in wirtschaftlichen Verhandlungen oder der Aufteilung von Ressourcen \cite[S. 5572]{ijcai2022p778}.

Das Verständnis der kooperativen Spieltheorie und ihrer Anwendung in Form der Shapley-Werte ermöglicht es, dieses theoretische Konzept 
auf den Bereich des maschinellen Lernens zu übertragen. Im Kontext des maschinellen Lernens wird das Spiel zur Modellvorhersage einer konkreten
Beobachtung. Die Spieler werden als Eingabemerkmale aufgefasst und bilden eine Koalition. Der Gewinn resultiert aus der Differenz einer Vorhersage und der durchschnittlichen Vorhersage des
Modells über alle Beobachtungen \cite[S. 215]{Molnar_2022}. Die charakteristische Funktion beschreibt die Gesamtleistung, 
die eine Kombination von Merkmalen auf die Vorhersage des Modells ausübt \cite[S. 5572]{ijcai2022p778}.

\section{Shapley-Werte, SHAP, SHAP-Werte und \textsf{shap}}

Zur Verdeutlichung und Abgrenzung der verschiedenen, jedoch verwandten Begrifflichkeiten, die im Kontext dieser Arbeit Verwendung finden, 
ist eine kurze Einordnung essenziell.

Beginnend mit den Shapley-Werten, entstammt dieser Begriff der kooperativen Spieltheorie und beschreibt eine Methode, 
um den fairen Beitrag eines Spielers zu der Gesamtauszahlung eines kooperativen Spiels zu bestimmen. 

SHAP (SHapley Additive exPlanations) ist ein Interpretationsframework für maschinelles Lernen, 
das Shapley-Werte verwendet, um den Beitrag jedes Merkmals zur Modellvorhersage 
zu quantifizieren. Dieses Konzept wurde erstmals von Lundberg und Lee 2017 eingeführt und 
ermöglicht die Erklärung komplexer Modelle und die Interpretation von Vorhersagen \cite[S. 1]{NIPS2017_8a20a862}.

Bei den SHAP-Werten handelt es sich um die konkreten quantitativen Beiträge der einzelnen Merkmale 
zu einer bestimmten Vorhersage, berechnet basierend auf dem SHAP-Framework.

Das Python-Paket \textsf{shap} schließlich ist eine Implementierung, die es praktikabel macht, SHAP-Werte in der Anwendung zu berechnen 
und zu visualisieren. Es stellt eine reiche Auswahl an Werkzeugen zur Verfügung, um diese Werte und ihre Auswirkungen zu interpretieren \cite[S. 14]{Molnar_2023}.



