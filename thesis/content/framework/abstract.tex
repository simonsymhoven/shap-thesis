\chapter*{Abstract\markboth{Abstract}{}}
\thispagestyle{empty}

In der vorliegenden Masterarbeit wird die Interpretation linearer Modelle mittels SHAP untersucht, 
einem Verfahren, das seine Grundlagen in den Shapley-Werten der kooperativen Spieltheorie findet. 
Die Untersuchung beginnt mit einem Rückblick auf die historische Entwicklung der Shapley-Werte und 
setzt sich mit einer detaillierten Herleitung sowie einer Analyse der zugrundeliegenden axiomatischen Prinzipien fort.
Es wird dargelegt, wie SHAP auf Basis der Shapley-Werte konzipiert wurde, um Beiträge der einzelnen Merkmale zur Vorhersagegenauigkeit 
eines Modells zu quantifizieren. 
Die Anwendung des Konzepts auf einen Datensatz aus der Praxis, bearbeitet mit dem Python-Paket \textsf{shap}, 
verdeutlicht die Handhabung und praktische Relevanz des Ansatzes. 
Den Schlussstein der Arbeit bildet ein Abgleich der gewonnenen Einsichten durch 
SHAP mit traditionellen Methoden zur Bestimmung der Relevanz von Modellmerkmalen. 
Zudem wird eine kritische Betrachtung der Grenzen und Herausforderungen, die mit SHAP einhergehen, präsentiert.