\chapter*{Abstract\markboth{Abstract}{}}
\thispagestyle{empty}

In der vorliegenden Masterarbeit wird die Interpretation linearer Modelle mittels SHAP untersucht, 
einem Verfahren, das seine Grundlagen in den Shapley-Werten der kooperativen Spieltheorie findet. 
Die Untersuchung beginnt mit einem Rückblick auf die historische Entwicklung der Shapley-Werte und 
setzt sich mit einer detaillierten Herleitung sowie einer Analyse der zugrundeliegenden axiomatischen Prinzipien fort.
Es wird dargelegt, wie SHAP auf Basis der Shapley-Werte konzipiert wurde, um Beiträge der einzelnen Merkmale zur Vorhersagegenauigkeit 
eines Modells zu quantifizieren. 
Die Anwendung des Konzepts auf einen Datensatz aus der Praxis, bearbeitet mit dem Python-Paket \textsf{shap}, 
verdeutlicht die Handhabung und praktische Relevanz des Ansatzes. 
Den Schlussstein der Arbeit bildet ein Abgleich der gewonnenen Einsichten durch 
SHAP mit traditionellen Methoden zur Bestimmung der Relevanz von Modellmerkmalen. 
Zudem wird eine kritische Betrachtung der Grenzen und Herausforderungen, die mit SHAP einhergehen, präsentiert.
Ein wesentliches Ergebnis der vorliegenden Arbeit ist die Erkenntnis, dass SHAP eine tiefere und nuanciertere 
Analyse der Modellvorhersagen ermöglicht, indem es nicht nur globale, sondern auch 
individuelle Merkmalsbeiträge hervorhebt. Diese Fähigkeit, sowohl lokale als auch globale Interpretationen 
zu liefern, unterscheidet SHAP wesentlich von traditionellen Methoden zur Bestimmung der Merkmalsrelevanz.